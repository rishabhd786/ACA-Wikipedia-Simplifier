{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T11:35:19.37795Z","iopub.execute_input":"2021-06-20T11:35:19.378669Z","iopub.status.idle":"2021-06-20T11:35:19.417007Z","shell.execute_reply.started":"2021-06-20T11:35:19.378546Z","shell.execute_reply":"2021-06-20T11:35:19.416284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nimport tensorflow as tf\nimport pandas as pd \nfrom keras.layers import Input, LSTM, GRU, Dense, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:19.418096Z","iopub.execute_input":"2021-06-20T11:35:19.418529Z","iopub.status.idle":"2021-06-20T11:35:25.139851Z","shell.execute_reply.started":"2021-06-20T11:35:19.418492Z","shell.execute_reply":"2021-06-20T11:35:25.138931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nor_train=open(\"/kaggle/input/test-dataset/normal.training.txt\",\"r\")\nsim_train=open(\"/kaggle/input/test-dataset/simple.training.txt\",\"r\")\nnormal_train=nor_train.readlines()\nsimple_train=sim_train.readlines()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:25.143643Z","iopub.execute_input":"2021-06-20T11:35:25.143927Z","iopub.status.idle":"2021-06-20T11:35:25.809056Z","shell.execute_reply.started":"2021-06-20T11:35:25.143901Z","shell.execute_reply":"2021-06-20T11:35:25.808335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_sentence(w):\n  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n  w = re.sub(r'[\" \"]+', \" \", w)\n  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n  w = w.strip()\n  w = 'sssss ' + w + ' eeeee'\n  return w","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:25.810655Z","iopub.execute_input":"2021-06-20T11:35:25.811247Z","iopub.status.idle":"2021-06-20T11:35:25.817257Z","shell.execute_reply.started":"2021-06-20T11:35:25.811205Z","shell.execute_reply":"2021-06-20T11:35:25.816321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1000\nEPOCHS = 64\nNUM_SENTENCES = 1000\nMAX_SENTENCE_LENGTH = 1200\nMAX_NUM_WORDS = 4000\nEMBEDDING_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:25.818643Z","iopub.execute_input":"2021-06-20T11:35:25.819037Z","iopub.status.idle":"2021-06-20T11:35:25.829984Z","shell.execute_reply.started":"2021-06-20T11:35:25.818998Z","shell.execute_reply":"2021-06-20T11:35:25.829058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sentences = []\noutput_sentences = []\noutput_sentences_inputs = []\ni=0\nk=0\nwhile i <(NUM_SENTENCES):\n  try:\n    s1=normal_train[k]\n  except:\n    print(k)\n    break\n  if(len(s1)<MAX_SENTENCE_LENGTH):\n    input_sentences.append(s1)\n    i=i+1\n    sentence = simple_train[k]\n    output_sentences.append(sentence)\n  k=k+1","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:25.83127Z","iopub.execute_input":"2021-06-20T11:35:25.831684Z","iopub.status.idle":"2021-06-20T11:35:25.948388Z","shell.execute_reply.started":"2021-06-20T11:35:25.831645Z","shell.execute_reply":"2021-06-20T11:35:25.94746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(input_sentences)):\n    input_sentences[i]=preprocess_sentence(input_sentences[i])\nfor i in range(len(output_sentences)):\n    output_sentences[i]=preprocess_sentence(output_sentences[i])\nlen(input_sentences)\nprint(input_sentences[:5])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:25.94968Z","iopub.execute_input":"2021-06-20T11:35:25.950071Z","iopub.status.idle":"2021-06-20T11:35:33.964421Z","shell.execute_reply.started":"2021-06-20T11:35:25.950032Z","shell.execute_reply":"2021-06-20T11:35:33.963299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenizer = Tokenizer(num_words=15000,oov_token='zzzzz')\ninput_tokenizer.fit_on_texts(input_sentences)\ninput_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\nword2idx_inputs = input_tokenizer.word_index\nprint('Total unique words in the input: %s' % len(word2idx_inputs))\nmax_input_len = max(len(sen) for sen in input_integer_seq)\nprint(\"Length of longest sentence in input: %g\" % max_input_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:33.966954Z","iopub.execute_input":"2021-06-20T11:35:33.967294Z","iopub.status.idle":"2021-06-20T11:35:39.389178Z","shell.execute_reply.started":"2021-06-20T11:35:33.967267Z","shell.execute_reply":"2021-06-20T11:35:39.388182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_tokenizer = Tokenizer(filters='',num_words=15000,oov_token='zzzzz')\noutput_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\noutput_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\nword2idx_outputs = output_tokenizer.word_index\nprint('Total unique words in the output: %s' % len(word2idx_outputs))\nnum_words_output = len(word2idx_outputs) + 1\nmax_out_len = max(len(sen) for sen in output_integer_seq)\nprint(\"Length of longest sentence in the output: %g\" % max_out_len)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:39.390841Z","iopub.execute_input":"2021-06-20T11:35:39.391121Z","iopub.status.idle":"2021-06-20T11:35:43.855762Z","shell.execute_reply.started":"2021-06-20T11:35:39.391095Z","shell.execute_reply":"2021-06-20T11:35:43.854647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_vocab=[]\noutput_vocab=[]\nfor key,val in word2idx_inputs.items():\n    input_vocab.append(key)\nfor key,val in word2idx_outputs.items():\n    output_vocab.append(key)\ninput_vocab=input_vocab[:15000]\noutput_vocab=output_vocab[:15000]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:43.863652Z","iopub.execute_input":"2021-06-20T11:35:43.86392Z","iopub.status.idle":"2021-06-20T11:35:43.913243Z","shell.execute_reply.started":"2021-06-20T11:35:43.863896Z","shell.execute_reply":"2021-06-20T11:35:43.912146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = pad_sequences(input_integer_seq, maxlen=max_input_len)\ntarget_tensor = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:43.918812Z","iopub.execute_input":"2021-06-20T11:35:43.919186Z","iopub.status.idle":"2021-06-20T11:35:46.027412Z","shell.execute_reply.started":"2021-06-20T11:35:43.919152Z","shell.execute_reply":"2021-06-20T11:35:46.026676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:46.028446Z","iopub.execute_input":"2021-06-20T11:35:46.02887Z","iopub.status.idle":"2021-06-20T11:35:46.742761Z","shell.execute_reply.started":"2021-06-20T11:35:46.028841Z","shell.execute_reply":"2021-06-20T11:35:46.741808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 1000\nsteps_per_epoch = len(input_tensor_train)/BATCH_SIZE\nembedding_dim = 100\nunits = 1024\nvocab_inp_size = 15001\nvocab_out_size = 15001\nsteps_per_epoch=int(steps_per_epoch)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:46.744012Z","iopub.execute_input":"2021-06-20T11:35:46.744287Z","iopub.status.idle":"2021-06-20T11:35:46.749758Z","shell.execute_reply.started":"2021-06-20T11:35:46.744261Z","shell.execute_reply":"2021-06-20T11:35:46.748581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:46.769135Z","iopub.execute_input":"2021-06-20T11:35:46.769401Z","iopub.status.idle":"2021-06-20T11:35:46.922423Z","shell.execute_reply.started":"2021-06-20T11:35:46.769375Z","shell.execute_reply":"2021-06-20T11:35:46.921536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:46.935251Z","iopub.execute_input":"2021-06-20T11:35:46.935837Z","iopub.status.idle":"2021-06-20T11:35:47.297274Z","shell.execute_reply.started":"2021-06-20T11:35:46.935794Z","shell.execute_reply":"2021-06-20T11:35:47.296585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nb_file=open(\"/kaggle/input/embeddings-dict/data.pkl\",\"rb\")\nembeddings_dict = pickle.load(b_file)\n# print(embeddings_dict)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:47.298377Z","iopub.execute_input":"2021-06-20T11:35:47.298806Z","iopub.status.idle":"2021-06-20T11:35:47.627512Z","shell.execute_reply.started":"2021-06-20T11:35:47.298766Z","shell.execute_reply":"2021-06-20T11:35:47.626628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_dict[0]=np.zeros([100])\nembeddings_dict[1]=np.zeros([100])#Make this an array of 1s\nembeddings_dict[2]=np.zeros([100])#Make this an array of -1s\n# embeddings_dict[3]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:47.638903Z","iopub.execute_input":"2021-06-20T11:35:47.639287Z","iopub.status.idle":"2021-06-20T11:35:47.645764Z","shell.execute_reply.started":"2021-06-20T11:35:47.639246Z","shell.execute_reply":"2021-06-20T11:35:47.644913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_input = np.zeros((15001, embedding_dim))\nfor word, i in word2idx_inputs.items():\n    embedding_vector = embeddings_dict.get(i)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix_input[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:47.648879Z","iopub.execute_input":"2021-06-20T11:35:47.649134Z","iopub.status.idle":"2021-06-20T11:35:47.704236Z","shell.execute_reply.started":"2021-06-20T11:35:47.64911Z","shell.execute_reply":"2021-06-20T11:35:47.703383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nb_file=open(\"/kaggle/input/embeddings-dict/data1.pkl\",\"rb\")\nembeddings_dict = pickle.load(b_file)\n# print(embeddings_dict)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:47.714462Z","iopub.execute_input":"2021-06-20T11:35:47.715044Z","iopub.status.idle":"2021-06-20T11:35:48.045205Z","shell.execute_reply.started":"2021-06-20T11:35:47.715004Z","shell.execute_reply":"2021-06-20T11:35:48.044504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_dict[0]=np.zeros([100])\nembeddings_dict[1]=np.zeros([100])#Make this an array of 1s\nembeddings_dict[2]=np.zeros([100])#Make this an array of -1s\n# embeddings_dict[3]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.056283Z","iopub.execute_input":"2021-06-20T11:35:48.056843Z","iopub.status.idle":"2021-06-20T11:35:48.06287Z","shell.execute_reply.started":"2021-06-20T11:35:48.056802Z","shell.execute_reply":"2021-06-20T11:35:48.061875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_output = np.zeros((15001, embedding_dim))\nfor word, i in word2idx_outputs.items():\n    embedding_vector = embeddings_dict.get(i)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix_output[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.064224Z","iopub.execute_input":"2021-06-20T11:35:48.06478Z","iopub.status.idle":"2021-06-20T11:35:48.120109Z","shell.execute_reply.started":"2021-06-20T11:35:48.064736Z","shell.execute_reply":"2021-06-20T11:35:48.119092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(\n    vocab_size,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_input),\n    trainable=False,\n)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n  def call(self, x, hidden):\n    #x = embeddings_dict(x)\n    x=self.embedding(x)\n    output, state = self.gru(x, initial_state=hidden)\n    return output, state\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.129957Z","iopub.execute_input":"2021-06-20T11:35:48.130243Z","iopub.status.idle":"2021-06-20T11:35:48.139407Z","shell.execute_reply.started":"2021-06-20T11:35:48.130216Z","shell.execute_reply":"2021-06-20T11:35:48.13853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\ntester= tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)\ntester(example_input_batch)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.140571Z","iopub.execute_input":"2021-06-20T11:35:48.141016Z","iopub.status.idle":"2021-06-20T11:35:48.784513Z","shell.execute_reply.started":"2021-06-20T11:35:48.140984Z","shell.execute_reply":"2021-06-20T11:35:48.783846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n  def call(self, query, values):\n    query_with_time_axis = tf.expand_dims(query, 1)\n    score = self.V(tf.nn.tanh(\n        self.W1(query_with_time_axis) + self.W2(values)))\n    attention_weights = tf.nn.softmax(score, axis=1)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.785416Z","iopub.execute_input":"2021-06-20T11:35:48.785792Z","iopub.status.idle":"2021-06-20T11:35:48.792426Z","shell.execute_reply.started":"2021-06-20T11:35:48.785765Z","shell.execute_reply":"2021-06-20T11:35:48.791697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_layer = BahdanauAttention(10)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.793298Z","iopub.execute_input":"2021-06-20T11:35:48.79375Z","iopub.status.idle":"2021-06-20T11:35:48.841355Z","shell.execute_reply.started":"2021-06-20T11:35:48.793723Z","shell.execute_reply":"2021-06-20T11:35:48.840433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(\n    vocab_size,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_output),\n    trainable=False,\n)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n    self.attention = BahdanauAttention(self.dec_units)\n  def call(self, x, hidden, enc_output):\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n    x = self.embedding(x)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    output, state = self.gru(x)\n    output = tf.reshape(output, (-1, output.shape[2]))\n    x = self.fc(output)\n    return x, state, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.842672Z","iopub.execute_input":"2021-06-20T11:35:48.842956Z","iopub.status.idle":"2021-06-20T11:35:48.852876Z","shell.execute_reply.started":"2021-06-20T11:35:48.842928Z","shell.execute_reply":"2021-06-20T11:35:48.851856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(vocab_out_size, embedding_dim, units, BATCH_SIZE)\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:48.854057Z","iopub.execute_input":"2021-06-20T11:35:48.854328Z","iopub.status.idle":"2021-06-20T11:35:49.143282Z","shell.execute_reply.started":"2021-06-20T11:35:48.854303Z","shell.execute_reply":"2021-06-20T11:35:49.142529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:49.144422Z","iopub.execute_input":"2021-06-20T11:35:49.14485Z","iopub.status.idle":"2021-06-20T11:35:49.150924Z","shell.execute_reply.started":"2021-06-20T11:35:49.144804Z","shell.execute_reply":"2021-06-20T11:35:49.150341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:49.151846Z","iopub.execute_input":"2021-06-20T11:35:49.15221Z","iopub.status.idle":"2021-06-20T11:35:49.164808Z","shell.execute_reply.started":"2021-06-20T11:35:49.152184Z","shell.execute_reply":"2021-06-20T11:35:49.164019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(inp, targ, enc_hidden):\n  print(\"Enter\")\n  loss = 0\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([output_tokenizer.word_index['sssss']] * BATCH_SIZE, 1)\n    for t in range(1, targ.shape[1]):\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n      loss += loss_function(targ[:, t], predictions)\n      dec_input = tf.expand_dims(targ[:, t], 1)\n      print(\"a\",end=\" \")\n  batch_loss = (loss / int(targ.shape[1]))\n  variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply_gradients(zip(gradients, variables))\n  print(\"4 done\")\n  return batch_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:35:49.16584Z","iopub.execute_input":"2021-06-20T11:35:49.166115Z","iopub.status.idle":"2021-06-20T11:35:49.17568Z","shell.execute_reply.started":"2021-06-20T11:35:49.16609Z","shell.execute_reply":"2021-06-20T11:35:49.174899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nEPOCHS = 1\nfor epoch in range(EPOCHS):\n  start = time.time()\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    print(\"out\")\n    total_loss += batch_loss\n    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n  checkpoint.save(file_prefix = checkpoint_prefix)\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:40:56.377246Z","iopub.execute_input":"2021-06-20T11:40:56.377634Z","iopub.status.idle":"2021-06-20T11:46:32.421268Z","shell.execute_reply.started":"2021-06-20T11:40:56.377597Z","shell.execute_reply":"2021-06-20T11:46:32.418908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef evaluate(sentence):\n  attention_plot = np.zeros((max_out_len, max_input_len))\n  sentence = preprocess_sentence(sentence)\n  inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_input_len,\n                                                         padding='post')\n  inputs = tf.convert_to_tensor(inputs)\n  result = ''\n  hidden = [tf.zeros((1, units))]\n  enc_out, enc_hidden = encoder(inputs, hidden)\n  dec_hidden = enc_hidden\n  dec_input = tf.expand_dims([output_tokenizer.word_index['sssss']], 0)\n  for t in range(max_out_len):\n    predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    attention_plot[t] = attention_weights.numpy()\n    predicted_id = tf.argmax(predictions[0]).numpy()\n    result += output_tokenizer.index_word[predicted_id] + ' '\n    if output_tokenizer.index_word[predicted_id] == 'eeeee':\n      return result, sentence, attention_plot\n    dec_input = tf.expand_dims([predicted_id], 0)\n  return result, sentence, attention_plot","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:36:59.631211Z","iopub.execute_input":"2021-06-20T11:36:59.6315Z","iopub.status.idle":"2021-06-20T11:36:59.642502Z","shell.execute_reply.started":"2021-06-20T11:36:59.631458Z","shell.execute_reply":"2021-06-20T11:36:59.641524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(sentence):\n  result, sentence, attention_plot = evaluate(sentence)\n  print('Input: %s' % (sentence))\n  print('Predicted translation: {}'.format(result))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:36:59.64371Z","iopub.execute_input":"2021-06-20T11:36:59.644076Z","iopub.status.idle":"2021-06-20T11:36:59.655931Z","shell.execute_reply.started":"2021-06-20T11:36:59.644048Z","shell.execute_reply":"2021-06-20T11:36:59.65521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:39:51.583965Z","iopub.execute_input":"2021-06-20T11:39:51.584333Z","iopub.status.idle":"2021-06-20T11:39:51.590669Z","shell.execute_reply.started":"2021-06-20T11:39:51.584298Z","shell.execute_reply":"2021-06-20T11:39:51.589591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(u\"the\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T11:38:26.07654Z","iopub.execute_input":"2021-06-20T11:38:26.07702Z","iopub.status.idle":"2021-06-20T11:38:31.687804Z","shell.execute_reply.started":"2021-06-20T11:38:26.07699Z","shell.execute_reply":"2021-06-20T11:38:31.686977Z"},"trusted":true},"execution_count":null,"outputs":[]}]}