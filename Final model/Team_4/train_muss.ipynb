{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MUSS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eDgjjNeBXulf",
        "VVkUDfv0TDNB",
        "oRYaxhueUpCV",
        "h3ZtlqW0Uu7r"
      ],
      "authorship_tag": "ABX9TyPjNHiywYAO3h/Q9OVtiJo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ris27hav/ACA-Wikipedia-Simplifier/blob/main/Final%20model/Team_4/train_muss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orOMOT86WBro"
      },
      "source": [
        "# Wikipedia Simplifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF1-rHc2WKV3"
      },
      "source": [
        "This is a simplified (xD) implementation of the MUSS model presented in the paper [MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases](https://arxiv.org/pdf/2005.00352.pdf) using *Pytorch* and *transformers* library. I have implemented it for English language only. This is not the exact implementation as described in the paper but similar to it. \n",
        "\n",
        "Since it is nearly impossible to train this model using Colab or using just 1 GPU and limited storage, so I have used the pretrained model for evaluations. Though the functions can be tested invidually to know if they work the way they are intended to do. Please check the other notebook to see the model in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSVo7VVpVhE"
      },
      "source": [
        "###Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9fNXCjOiVh5"
      },
      "source": [
        "####**1. Mining Paraphrases**\n",
        "\n",
        ">**a) Sequence Extraction**:\n",
        ">A sequence consists of multiple sentences: to allow sentence splitting or fusion operations.\n",
        "\n",
        "* get docs from HEAD split in CCNet (categorised by language)\n",
        "* doc -> sentences using NLTK sentence tokenizer\n",
        "* adjacent sentences -> sequences with max character = 300\n",
        "* filter sequences: remove those having >= 10% puctutation characters or low language model probability according to a 3-gram Kneser-Ney language model trained with kenlm on Wikipedia\n",
        "\n",
        "\n",
        ">**b) Creating a Sequence Index using Embeddings**\n",
        "\n",
        "* extracted sequence (1 billion) -> 1024-dimensional embeddings using LASER (reduced to 512 by using PCA followed by random rotation)\n",
        "\n",
        "\n",
        ">**c) Mining paraphrases**\n",
        "\n",
        "* Index these embeddings (for use with faiss)\n",
        "* each sequence is used as a query (q_i) against these 1 billion sequences to find top 8 nearest neighbour using L2 distance (faiss), keep those with L2 distance < 0.05 and relative distance with other 7 neighbours < 0.6\n",
        "* paraphrase filtering -<br>\n",
        "    - remove almost identical pp with character Levenshtein distance <= 20%\n",
        "    - remove pp coming from same document\n",
        "    - remove pp where one sqeuence is contained in other\n",
        "\n",
        "\n",
        "####**2. Simplifying with ACCESS**\n",
        "\n",
        "* ACCESS is a method to make any seq2seq model controllable by conditioning on simplification-specific control tokens.\n",
        "* Apply this to seq2seq pretrained transformer models based on the BART.\n",
        "\n",
        ">**Training with control tokens**\n",
        "\n",
        "- During train time, control tokens provided to model that give info about target sequence.\n",
        "- During inference time, control the generation by selecting a given target control value.\n",
        "- Prepend the following control tokens to every source in training set:\n",
        "\t\t<NumChars_XX%> : Character Length Ratio\n",
        "\t\t<LevSim_YY%> : replace-only Levenshtein similarity\n",
        "\t\t<WordFreq_ZZ%> : aggregated word frequency ratio\n",
        "\t\t<DepTreeDepth_TT%> : dependency tree depth ratio\n",
        "\n",
        ">**Selecting Control values at Inference**\n",
        "\n",
        "- Shorter sentences are more adapted to people with cognitive disabilities, while using more frequent words are useful to second language learners.\n",
        "- Choose these hyperparameters based on SARI score on validation set or by using prior knowledge based on target audience.\n",
        "\n",
        "\n",
        "####**3. Leveraging Unsupervised Pretraining**\n",
        "\n",
        "* Fine tune the pretrained generative model BART on the newly created training corpora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eI3uQh0aKz9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDgjjNeBXulf"
      },
      "source": [
        "### Loading the data for mining paraphrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCdgWwrClbSb"
      },
      "source": [
        "**Get data from CCNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHFOuBZggHJr",
        "outputId": "c8d2f9fb-149c-49ed-9bec-1cb9b3cc8bf3"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/cc_net"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cc_net'...\n",
            "remote: Enumerating objects: 471, done.\u001b[K\n",
            "remote: Total 471 (delta 0), reused 0 (delta 0), pack-reused 471\u001b[K\n",
            "Receiving objects: 100% (471/471), 169.97 KiB | 4.59 MiB/s, done.\n",
            "Resolving deltas: 100% (329/329), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-01DRvtJgRCa",
        "outputId": "f3d90067-730b-448d-b721-d65bd705cb86"
      },
      "source": [
        "%cd cc_net/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cc_net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-YUQIQgND8"
      },
      "source": [
        "!mkdir ./data/"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnwZzglgjpU"
      },
      "source": [
        "!python -m pip install .[getpy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irp46hVsfxdZ"
      },
      "source": [
        "# !python -m cc_net --dump 2019-13\n",
        "\n",
        "# Note : this won't work here because it requires 7 TB storage :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmzTmKq1le5Y"
      },
      "source": [
        "**Get sample data from wikipedia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK0nxvZYleHY",
        "outputId": "b297d88f-189d-4dc5-d261-9fdf0496869d"
      },
      "source": [
        "# For testing purpose, we can provide some sample data\n",
        "%pip install wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=6b368574f746b01ed9852ed3179de0aabed882e83b71ba7dd02d8d0818eb250e\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNtWCLkmln_u"
      },
      "source": [
        "import wikipedia"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXU2spGTX3Ku"
      },
      "source": [
        "# upload simple_wiki.txt file\n",
        "\n",
        "def get_data():\n",
        "    titles = [\"Messi\", \"Christiano Ronaldo\", 'messi-ronaldo rivalry', 'chernobyl', 'artificial intelligence', 'Hinduism']\n",
        "    docs = []\n",
        "    with open('simple_wiki.txt') as fo:\n",
        "        data = fo.read()\n",
        "        docs.append(data)\n",
        "    for title in titles:\n",
        "        page = wikipedia.page(title)\n",
        "        docs.append(page.content)\n",
        "    return docs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVkUDfv0TDNB"
      },
      "source": [
        "### Mining Paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGE16C89V8sE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff3e220-9429-4f77-d9de-b26c8644a044"
      },
      "source": [
        "# Install required dependencies\n",
        "%pip install laserembeddings python-Levenshtein faiss faiss-cpu"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: laserembeddings in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post2)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /usr/local/lib/python3.7/dist-packages (from laserembeddings) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from laserembeddings) (1.19.5)\n",
            "Requirement already satisfied: transliterate==1.10.2 in /usr/local/lib/python3.7/dist-packages (from laserembeddings) (1.10.2)\n",
            "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from laserembeddings) (0.3.7)\n",
            "Requirement already satisfied: sacremoses==0.0.35 in /usr/local/lib/python3.7/dist-packages (from laserembeddings) (0.0.35)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->laserembeddings) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->laserembeddings) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->laserembeddings) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->laserembeddings) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT9PXDbXfu2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7defbdae-900e-4c66-be15-4d649b797929"
      },
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from laserembeddings import Laser\n",
        "from sklearn.decomposition import PCA\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsWEF4izTPi9"
      },
      "source": [
        "**Sequence Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnW6Sovgf1es"
      },
      "source": [
        "def doc_to_sentences(doc):\n",
        "    \"\"\"\n",
        "    Splits a document into sentences.\n",
        "    \"\"\"\n",
        "    doc = doc.replace('\\n', ' ').replace('\\t', ' ').replace('\\x00', ' ')\n",
        "    return sent_tokenize(doc)\n",
        "\n",
        "\n",
        "def filter_sentences(sentences, punc_ratio=0.1, lang_model=None, lang_prob=0.5):\n",
        "    \"\"\"\n",
        "    Filters sentences by removing those which contain a high number of punctuation marks \n",
        "    or have low language model probability or are too small.\n",
        "    \"\"\"\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for seq in sentences:\n",
        "        if len(seq) < 30:\n",
        "            continue\n",
        "\n",
        "        if lang_model is not None:\n",
        "            prob = lang_model.prob(seq)\n",
        "            if prob < lang_prob:\n",
        "                continue\n",
        "\n",
        "        num_punc = sum(1 for c in seq if c in punctuation)\n",
        "        if num_punc / len(seq) < punc_ratio:\n",
        "            filtered_sentences.append(seq)\n",
        "\n",
        "    return filtered_sentences\n",
        "\n",
        "\n",
        "def generate_sequences(sentences, max_chars=300):\n",
        "    \"\"\"\n",
        "    Generates sequences of adjacent sentences from a list of sentences.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    for i in range(total_sentences):\n",
        "        cur_seq = sentences[i]\n",
        "        cur_chars = len(cur_seq)\n",
        "        if cur_chars > max_chars:\n",
        "            continue\n",
        "        \n",
        "        sequences.append(cur_seq)\n",
        "        for j in range(i+1, total_sentences):\n",
        "            cur_chars += len(sentences[j])\n",
        "            if cur_chars > max_chars:\n",
        "                break\n",
        "            \n",
        "            cur_seq += ' ' + sentences[j]\n",
        "            sequences.append(cur_seq)\n",
        "\n",
        "    return sequences"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56payAETsln"
      },
      "source": [
        "**Creating a sequence index using embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfesZM9zT9e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e673db62-7305-4ae8-8cd3-fbcc6a5d1483"
      },
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading models into /usr/local/lib/python3.7/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf49FUnMT2WN"
      },
      "source": [
        "def compute_embeddings(sequences, dim=512):\n",
        "    \"\"\"\n",
        "    Computes the embeddings for a list of sequences.\n",
        "    \"\"\"\n",
        "    laser = Laser()\n",
        "    embeddings = laser.embed_sentences(sequences, lang='en')\n",
        "    # embeddings is a N*1024 (N = number of sentences) NumPy array\n",
        "\n",
        "    pca = PCA(n_components=dim)\n",
        "    embeddings = pca.fit_transform(embeddings)\n",
        "    return embeddings"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLWBcLoNTimw"
      },
      "source": [
        "**Mining Paraphrases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsHaWVUTlnf"
      },
      "source": [
        "def index_embeddings(embeddings):\n",
        "    \"\"\"\n",
        "    Indexes a list of embeddings using a FAISS index.\n",
        "    \"\"\"\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "\n",
        "def get_nearest_neighbors(index, embeddings, k=8):\n",
        "    \"\"\"\n",
        "    Returns the k nearest neighbors of each embedding in a list of embeddings.\n",
        "    \"\"\"\n",
        "    D, I = index.search(embeddings, k)\n",
        "    return D, I\n",
        "\n",
        "\n",
        "def filter_nearest_neighbors(D, I, max_L2_dist=0.05):\n",
        "    \"\"\"\n",
        "    Filters the nearest neighbors to remove those which are too far from the queries.\n",
        "    \"\"\"\n",
        "    filtered_neighbors = np.ones(I.shape) * (-1)\n",
        "    for i in range(I.shape[0]):\n",
        "        for j in range(I.shape[1]):\n",
        "            if D[i,j] <= max_L2_dist:\n",
        "                filtered_neighbors[i,j] = I[i,j]\n",
        "    \n",
        "    filtered_neighbors = filtered_neighbors.astype(int)\n",
        "    return filtered_neighbors\n",
        "\n",
        "\n",
        "def filter_paraphrases(I, sequences, min_l_dist=0.2):\n",
        "    \"\"\"\n",
        "    Removes almost identical pp with character level Levenshtein distance <= 20%\n",
        "\tor pp from coming same document         ** (need to implement this) **\n",
        "\tor pp where one sequence is contained in other\n",
        "    \"\"\"\n",
        "    for i in range(I.shape[0]):\n",
        "        cur_seq = sequences[i]\n",
        "        for j in range(I.shape[1]):\n",
        "            if I[i,j] == -1:\n",
        "                continue\n",
        "            \n",
        "            target_seq = sequences[I[i,j]]\n",
        "            dist = levenshtein_distance(cur_seq, target_seq)\n",
        "            if dist <= min_l_dist:\n",
        "                I[i,j] = -1\n",
        "                continue\n",
        "            \n",
        "            if cur_seq in target_seq or target_seq in cur_seq:\n",
        "                I[i,j] = -1\n",
        "\n",
        "    return I\n",
        "\n",
        "\n",
        "def generate_aligned_paraphrases(I, sequences):\n",
        "    \"\"\"\n",
        "    Generates a list of paraphrases from the list of sequences and their nearest neighbors.\n",
        "    \"\"\"\n",
        "    paraphrases = []\n",
        "    for i in range(I.shape[0]):\n",
        "        cur_seq = sequences[i]\n",
        "        for j in range(I.shape[1]):\n",
        "            if I[i,j] == -1:\n",
        "                continue\n",
        "            \n",
        "            target_seq = sequences[I[i,j]]\n",
        "            paraphrases.append((cur_seq, target_seq))\n",
        "    \n",
        "    return paraphrases"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzsTvRcmZr6u"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRYaxhueUpCV"
      },
      "source": [
        "### Simplifying with ACCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnzg7qEUZfpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989a9121-ea45-4c5a-f5a6-fceebab58c3b"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FfOGSIDiBLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39bace1-1a9e-4ec5-e069-631f874bea46"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-31 19:06:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.1’\n",
            "\n",
            "cc.en.300.vec.gz.1  100%[===================>]   1.23G  24.2MB/s    in 53s     \n",
            "\n",
            "2021-07-31 19:07:52 (23.8 MB/s) - ‘cc.en.300.vec.gz.1’ saved [1325960915/1325960915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1luRjujgh93o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bcfb530-298a-4c1d-cde5-f30fe3b55259"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJBbY7MjZaoZ"
      },
      "source": [
        "import Levenshtein\n",
        "import spacy\n",
        "import numpy as np\n",
        "import gzip\n",
        "import en_core_web_md"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ZqmcslYIt7"
      },
      "source": [
        "**Character length Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfVb0Qj9YQ3B"
      },
      "source": [
        "def get_character_length_ratio(original_seq, target_seq):\n",
        "    \"\"\"\n",
        "    Return the ratio (in %) of the length of the target sequence\n",
        "    to the length of the original sequence.\n",
        "    \"\"\"\n",
        "    return (len(target_seq) / len(original_seq)) * 100"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7md-JcUYGae"
      },
      "source": [
        "**Replace-Only Levenshtein Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rpJMeEzYR-T"
      },
      "source": [
        "def get_replace_only_levenshtein_similarity(original_seq, target_seq):\n",
        "    \"\"\"\n",
        "    Return the ratio (in %) of the Levenshtein distance between the target \n",
        "    sequence and the original sequence, where only replacements are considered.\n",
        "    \"\"\"\n",
        "    distance = len(\n",
        "        [\n",
        "            _\n",
        "            for operation, _, _ in Levenshtein.editops(original_seq, target_seq)\n",
        "            if operation == \"replace\"\n",
        "        ]\n",
        "    )\n",
        "    max_replace_only_distance = min(len(original_seq), len(target_seq))\n",
        "    if max_replace_only_distance == 0:\n",
        "        return 0\n",
        "    return (1 - (distance / max_replace_only_distance)) * 100"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfCOaryAX7-M"
      },
      "source": [
        "**Word Frequency Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT3FjxJ8YX7P"
      },
      "source": [
        "def yield_lines():\n",
        "    with gzip.open(\"./cc.en.300.vec.gz\", \"rt\") as f:\n",
        "        for line in f:\n",
        "            yield line.rstrip('\\n')\n",
        "\n",
        "def get_word2rank(vocab_size=60000):\n",
        "    word2rank = {}\n",
        "    line_generator = yield_lines()\n",
        "    next(line_generator)\n",
        "    for i, line in enumerate(line_generator):\n",
        "        if (i + 1) > vocab_size:\n",
        "            break\n",
        "        word = line.split(\" \")[0]\n",
        "        word2rank[word] = i\n",
        "    return word2rank\n",
        "\n",
        "def is_content_token(token):\n",
        "    return not token.is_stop and not token.is_punct and token.ent_type_ == ''  # Not named entity\n",
        "\n",
        "def get_content_words(text, spacy_model):\n",
        "    spacy_tokenizer = spacy_model.Defaults.create_tokenizer(spacy_model)\n",
        "    spacy_content_tokens = [token for token in spacy_tokenizer(text) if is_content_token(token)]\n",
        "    return [token.text for token in spacy_content_tokens]\n",
        "\n",
        "def get_log_ranks(text, spacy_model, word2rank):\n",
        "    return [\n",
        "        np.log(1 + word2rank.get(word, len(word2rank)))\n",
        "        for word in get_content_words(text, spacy_model)\n",
        "        if word in word2rank\n",
        "    ]\n",
        "\n",
        "def get_word_rank_ratio(original_seq, target_seq, spacy_model, word2rank):\n",
        "    \"\"\"\n",
        "    Return the ratio (in %) of the word rank of the target sequence\n",
        "    to the word rank of the original sequence.\n",
        "    \"\"\"    \n",
        "    orig_log_ranks = get_log_ranks(original_seq, spacy_model, word2rank)\n",
        "    target_log_ranks = get_log_ranks(target_seq, spacy_model, word2rank)\n",
        "    if len(orig_log_ranks) == 0:\n",
        "        orig_log_ranks = [np.log(1 + len(word2rank))]\n",
        "    if len(target_log_ranks) == 0:\n",
        "        target_log_ranks = [np.log(1 + len(word2rank))]\n",
        "    \n",
        "    orig_log_rank = np.quantile(orig_log_ranks, 0.75)\n",
        "    target_log_rank = np.quantile(target_log_ranks, 0.75)\n",
        "    \n",
        "    return (target_log_rank / orig_log_rank) * 100"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H-HSX-nX2bh"
      },
      "source": [
        "**Dependency Tree Depth Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCKmAOQtUrwC"
      },
      "source": [
        "def get_subtree_depth(node):\n",
        "    if len(list(node.children)) == 0:\n",
        "        return 0\n",
        "    return 1 + max([get_subtree_depth(child) for child in node.children])\n",
        "\n",
        "\n",
        "def get_dependency_tree_depth_ratio(original_seq, target_seq, model):\n",
        "    \"\"\"\n",
        "    Return the ratio (in %) of the depth of the dependency tree of the target \n",
        "    sequence to the depth of the dependency tree of the original sequence.\n",
        "    \"\"\"\n",
        "    original_tree_depths = [\n",
        "        get_subtree_depth(spacy_sentence.root)\n",
        "        for spacy_sentence in model(str(original_seq)).sents\n",
        "    ]\n",
        "    target_tree_depths = [\n",
        "        get_subtree_depth(spacy_sentence.root)\n",
        "        for spacy_sentence in model(str(target_seq)).sents\n",
        "    ]\n",
        "    original_tree_depth = 0 if len(original_tree_depths) == 0 else max(original_tree_depths)\n",
        "    target_tree_depth = 0 if len(target_tree_depths) == 0 else max(target_tree_depths)\n",
        "\n",
        "    return 0 if original_tree_depth == 0 else (target_tree_depth / original_tree_depth) * 100"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "659_m7_pwboN"
      },
      "source": [
        "**Prepend the paraphrases with Control Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHfM0rw4wa8m"
      },
      "source": [
        "def prepend_control_tokens(paraphrases):\n",
        "    \"\"\"\n",
        "    Return the list of paraphrases where each original sequence is prepended\n",
        "    by the control tokens.\n",
        "    \"\"\"\n",
        "    spacy_model = en_core_web_md.load()\n",
        "    word2rank = get_word2rank(vocab_size=60000)\n",
        "    final_pps = []\n",
        "    for orig_seq, target_seq in paraphrases:\n",
        "        tokens = []\n",
        "        tokens.append(get_character_length_ratio(orig_seq, target_seq))\n",
        "        tokens.append(get_replace_only_levenshtein_similarity(orig_seq, target_seq))\n",
        "        tokens.append(get_word_rank_ratio(orig_seq, target_seq, spacy_model, word2rank))\n",
        "        tokens.append(get_dependency_tree_depth_ratio(orig_seq, target_seq, spacy_model))\n",
        "        \n",
        "        # Round the ratios in a fixed interval of 0.05 (5%) and capped to \n",
        "        # a maximum ratio of 2 (200%)\n",
        "        mod_tokens = []\n",
        "        for token in tokens:\n",
        "            token = round(token / 5) * 5\n",
        "            token = min(max(5, token), 200)\n",
        "            mod_tokens.append(token)\n",
        "\n",
        "        CTRL_TOKEN = \"<NbChars_{:.0f}%> <LevSim_{:.0f}%> <WordFreq_{:.0f}%> <DepTreeDepth_{:.0f}%> \".format(\n",
        "            mod_tokens[0], mod_tokens[1], mod_tokens[2], mod_tokens[3]\n",
        "        )\n",
        "        orig_seq = CTRL_TOKEN + orig_seq\n",
        "        final_pps.append((orig_seq, target_seq))\n",
        "        \n",
        "    return final_pps\n",
        "\n",
        "\n",
        "def prepend_control_tokens_for_inference(sentence, tokens):\n",
        "    \"\"\"\n",
        "    Return the sentence encoded with the control tokens for inference\n",
        "    \"\"\"\n",
        "    CTRL_TOKEN = \"<NbChars_{:.0f}%> <LevSim_{:.0f}%> <WordFreq_{:.0f}%> <DepTreeDepth_{:.0f}%> \".format(\n",
        "        tokens[0], tokens[1], tokens[2], tokens[3]\n",
        "    )\n",
        "    return CTRL_TOKEN + sentence"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ySYiYm7Zw2Q"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ZtlqW0Uu7r"
      },
      "source": [
        "### Leveraging Unsupervised Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP3kV1QUw9i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2829d073-a22d-4d92-b1be-b3a8934bed6b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH12F6pMxM1t"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2C7ZP-fxP-t"
      },
      "source": [
        "class PPDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for paraphrase generation\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.encodings = {'input_ids': [], 'labels': []}\n",
        "        for pp in data:\n",
        "            source, target = pp\n",
        "            source = tokenizer.encode(source)\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                target = tokenizer.encode(target)\n",
        "            self.encodings['input_ids'].append(source)\n",
        "            self.encodings['labels'].append(target)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = {key: val[index] for key, val in self.encodings.items()}\n",
        "        return item"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlDFoyi-c-Wf"
      },
      "source": [
        "def get_tokenizer():\n",
        "    \"\"\"\n",
        "    Return the pretrained BART tokenizer and add the control tokens to it\n",
        "    \"\"\"\n",
        "    tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large')\n",
        "    control_tokens = []\n",
        "    for token in ['NbChars', 'LevSim', 'WordFreq', 'DepTreeDepth']:\n",
        "        for i in range(5, 201, 5):\n",
        "            control_tokens.append(f'<{token}_{i}%>')\n",
        "    tokenizer.add_tokens(control_tokens)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_dataset(paraphrases, tokenizer):\n",
        "    \"\"\"\n",
        "    Create a dataset from the paraphrases\n",
        "    \"\"\"\n",
        "    dataset = PPDataset(paraphrases, tokenizer)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_model(vocab_size):\n",
        "    \"\"\"\n",
        "    Return the pretrained BART model and add fix the token embeddings matrix\n",
        "    \"\"\"\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "    model.resize_token_embeddings(vocab_size)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_training_arguments(epochs=10, batch_size=8):\n",
        "    \"\"\"\n",
        "    Rturn the training arguments\n",
        "    \"\"\"\n",
        "    args = Seq2SeqTrainingArguments(\n",
        "        output_dir = 'outputs',\n",
        "        learning_rate = 3e-5,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        weight_decay = 0.01,\n",
        "        num_train_epochs = epochs,\n",
        "        predict_with_generate = True\n",
        "    )\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_data_collator(tokenizer, model):\n",
        "    \"\"\"\n",
        "    Return the data collator for seq2seq model\n",
        "    \"\"\"\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def get_trainer(model, tokenizer, dataset, data_collator, training_arguments):\n",
        "    \"\"\"\n",
        "    Return the trainer for fine-tuning the pretrained BART model\n",
        "    \"\"\"\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model = model,\n",
        "        data_collator = data_collator,\n",
        "        args = training_arguments,\n",
        "        train_dataset = dataset,\n",
        "        tokenizer = tokenizer,\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def simplify(sentence, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Return the simplified sentence\n",
        "    \"\"\"\n",
        "    tokenized_sentence = tokenizer.encode(sentence, return_tensors='pt')\n",
        "    output = model.generate(tokenized_sentence, num_beams=5)\n",
        "    output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in output]\n",
        "    return output\n",
        "\n",
        "\n",
        "def decode_sentence(encoded_sentence, tokenizer):\n",
        "    \"\"\"\n",
        "    Decode the encoded sentence\n",
        "    \"\"\"\n",
        "    decoded_sentence = tokenizer.decode(encoded_sentence, skip_special_tokens=True)\n",
        "    return decoded_sentence"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52218GLZyU6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh1538ZJU4if"
      },
      "source": [
        "### Training in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhl3BOZULd4J"
      },
      "source": [
        "**Mine Paraphrases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml5L5usTLVah"
      },
      "source": [
        "data = get_data()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3MEGrPxLjhg"
      },
      "source": [
        "sentences = []\n",
        "for doc in data:\n",
        "    sentences += doc_to_sentences(doc)\n",
        "\n",
        "sentences = filter_sentences(sentences, punc_ratio=0.1, lang_model=None, lang_prob=0.5)\n",
        "sentences.append('This paragraph is tough to comprehend.')\n",
        "sentences.append('This paragraph is very hard to understand.')\n",
        "sequences = generate_sequences(sentences, max_chars=300)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aud7khFSNpPU"
      },
      "source": [
        "embeddings = np.ascontiguousarray(compute_embeddings(sequences, dim=512))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh03ARKhz0fO"
      },
      "source": [
        "emb_index = index_embeddings(embeddings)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-FdGLEUNm3K"
      },
      "source": [
        "D, I = get_nearest_neighbors(emb_index, embeddings, k=8)\n",
        "filtered_I = filter_nearest_neighbors(D, I, max_L2_dist=0.05)\n",
        "filtered_I = filter_paraphrases(filtered_I, sequences, min_l_dist=0.2)\n",
        "paraphrases = generate_aligned_paraphrases(filtered_I, sequences)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qssIpJ11XIv",
        "outputId": "dbff7fed-98d8-4103-f9fb-7ac12d941a8e"
      },
      "source": [
        "print(len(final_pps))\n",
        "for source, target in final_pps:\n",
        "    print('Simple:', source)\n",
        "    print('Complex:', target, '\\n')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> The term Hindu was later used in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts including Chaitanya Charitamrita and Chaitanya Bhagavata.\n",
            "Complex: The term Hindu was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts, including Chaitanya Charitamrita and Chaitanya Bhagavata. \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_120%> These texts used to distinguish Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century Chaitanya Charitamrita text and the 17th century Bhakta Mala text using the phrase \"Hindu dharma\".\n",
            "Complex: These texts used it to contrast Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century Chaitanya Charitamrita text and the 17th-century Bhakta Mala text using the phrase \"Hindu dharma\". \n",
            "\n",
            "Simple: <NbChars_70%> <LevSim_45%> <WordFreq_105%> <DepTreeDepth_125%> The weak overcomes the stronger by Dharma, as over a king. Truly that Dharma is the Truth (Satya); Therefore, when a man speaks the Truth, they say, \"He speaks the Dharma\"; and if he speaks Dharma, they say, \"He speaks the Truth!\"\n",
            "Complex: 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God. \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_30%> <WordFreq_105%> <DepTreeDepth_125%> Truly that Dharma is the Truth (Satya); Therefore, when a man speaks the Truth, they say, \"He speaks the Dharma\"; and if he speaks Dharma, they say, \"He speaks the Truth!\"\n",
            "Complex: 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God. \n",
            "\n",
            "Simple: <NbChars_200%> <LevSim_70%> <WordFreq_125%> <DepTreeDepth_200%> The soul is believed to be eternal.\n",
            "Complex: 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God. \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_65%> <WordFreq_95%> <DepTreeDepth_100%> The soul is believed to be eternal.\n",
            "Complex: No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_30%> <WordFreq_120%> <DepTreeDepth_100%> The soul is believed to be eternal.\n",
            "Complex: who also have their own messes. \n",
            "\n",
            "Simple: <NbChars_145%> <LevSim_45%> <WordFreq_95%> <DepTreeDepth_80%> 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God.\n",
            "Complex: The weak overcomes the stronger by Dharma, as over a king. Truly that Dharma is the Truth (Satya); Therefore, when a man speaks the Truth, they say, \"He speaks the Dharma\"; and if he speaks Dharma, they say, \"He speaks the Truth!\" \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_30%> <WordFreq_95%> <DepTreeDepth_80%> 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God.\n",
            "Complex: Truly that Dharma is the Truth (Satya); Therefore, when a man speaks the Truth, they say, \"He speaks the Dharma\"; and if he speaks Dharma, they say, \"He speaks the Truth!\" \n",
            "\n",
            "Simple: <NbChars_20%> <LevSim_70%> <WordFreq_80%> <DepTreeDepth_40%> 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God.\n",
            "Complex: The soul is believed to be eternal. \n",
            "\n",
            "Simple: <NbChars_25%> <LevSim_70%> <WordFreq_75%> <DepTreeDepth_40%> 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God.\n",
            "Complex: No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_20%> <LevSim_85%> <WordFreq_100%> <DepTreeDepth_40%> 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God.\n",
            "Complex: who also have their own messes. \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> They are two of the most decorated football players ever, having won a combined 66 trophies (Ronaldo 32, Messi 34) during their senior careers thus far, and have regularly broken the 50-goal barrier in a single season.\n",
            "Complex: They are two of the most decorated football players ever, having won a combined 67 trophies (Ronaldo 32, Messi 35) during their senior careers thus far, and have regularly broken the 50-goal barrier in a single season. \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's physical attributes, goalscoring skills, leadership and influence under pressure is well-appreciated worldwide.\n",
            "Complex: Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's leadership and influence under pressure is well-appreciated worldwide. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_85%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_85%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. In total, Messi and Ronaldo reached the podium a record twelve times each.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. In total, Messi and Ronaldo have both won the World Cup twelve times. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_95%> <DepTreeDepth_120%> Messi during this period won five Pichichi trophies and European Golden Shoe awards (2010, 2012, 2013, 2017 and 2018), while Ronaldo won these prizes thrice each (2011, 2014 and 2015).\n",
            "Complex: Messi during this period won five Pichichi trophies and European Golden Shoe awards (2010, 2012, 2013, 2017 and 2018), while Ronaldo won these prizes four times each (2008, 2011, 2014 and 2015). \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_100%> I think they have their own personal pride in terms of wanting to be the best.\" Messi has denied any rivalry, and blames the media for creating it, stating that \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\n",
            "Complex: I think they have their own personal pride in terms of wanting to be the best.\" Messi himself denied any rivalry, saying that it was \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_115%> Although it was a bit difficult to see him win trophies, he gave La Liga prestige.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\"\n",
            "Complex: It was a little hard for him to win trophies, but he gave La Liga importance.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_115%> During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\"\n",
            "Complex: During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> ---  == Awards and records == Throughout the existence of the rivalry, the pair have dominated awards ceremonies and broken a multitude of goalscoring records for both club and country, feats which have been described as \"incredible\", \"ridiculous\", and \"remarkable\", respectively.\n",
            "Complex: == Awards and records == Throughout the existence of the rivalry, the pair have dominated awards ceremonies and broken a multitude of goalscoring records for both club and country, feats which have been described as \"incredible\", \"ridiculous\" and \"remarkable\". \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level.\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_100%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level.\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. Nobody has the right to compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Moreover, Messi was runner-up at three Copa Américas and at the 2014 World Cup, before finally claiming his first major international trophy at the 2021 Copa América where he was named joint-best player and top scorer.\n",
            "Complex: Moreover, Messi was runner-up at three Copa Américas and at the 2014 World Cup, before finally claiming his first major international trophy at the 2021 Copa América where he was named best player and joint top scorer. \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's leadership and influence under pressure is well-appreciated worldwide.\n",
            "Complex: Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's physical attributes, goalscoring skills, leadership and influence under pressure is well-appreciated worldwide. \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's leadership and influence under pressure is well-appreciated worldwide.\n",
            "Complex: Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's physical attributes, goalscoring skills, leadership and influence under pressure is well-appreciated worldwide. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_120%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_120%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_120%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. In total, Messi and Ronaldo have both won the World Cup twelve times.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. In total, Messi and Ronaldo reached the podium a record twelve times each. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_120%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. In total, Messi and Ronaldo have both won the World Cup twelve times.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. In total, Messi and Ronaldo reached the podium a record twelve times each. \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_85%> <WordFreq_100%> <DepTreeDepth_175%> == Relationship between Messi and Ronaldo ==  In a 2015 interview, Ronaldo talked about the rivalry. He said: \"Sometimes we push each other a little bit, that's why the competition is so good.\n",
            "Complex: == Relationship between Messi and Ronaldo == In a 2015 interview, Ronaldo commented on the rivalry by saying: \"I think we push each other sometimes in the competition, this is why the competition is so high.\" \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_90%> It was a little hard for him to win trophies, but he gave La Liga importance.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\"\n",
            "Complex: Although it was a bit difficult to see him win trophies, he gave La Liga prestige.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_90%> It was a little hard for him to win trophies, but he gave La Liga importance.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\"\n",
            "Complex: Although it was a bit difficult to see him win trophies, he gave La Liga prestige.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_90%> During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\"\n",
            "Complex: During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_90%> During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\"\n",
            "Complex: During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo.\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo.\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. Nobody has the right to compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_120%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. Nobody has the right to compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_100%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_100%> <DepTreeDepth_150%> In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. \n",
            "\n",
            "Simple: <NbChars_75%> <LevSim_40%> <WordFreq_130%> <DepTreeDepth_100%> No one should compare themselves to them.\"\n",
            "Complex: who also have their own messes. \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_65%> <WordFreq_105%> <DepTreeDepth_100%> No one should compare themselves to them.\"\n",
            "Complex: The soul is believed to be eternal. \n",
            "\n",
            "Simple: <NbChars_200%> <LevSim_70%> <WordFreq_130%> <DepTreeDepth_200%> No one should compare themselves to them.\"\n",
            "Complex: 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God. \n",
            "\n",
            "Simple: <NbChars_125%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_120%> He has also won four national cups, two league cups, six national super cups, and four FIFA Club World Cups.\n",
            "Complex: He has also won four national cups, two league cups, six national super cups, two European Super Cups, and four FIFA Club World Cups. \n",
            "\n",
            "Simple: <NbChars_135%> <LevSim_40%> <WordFreq_80%> <DepTreeDepth_100%> who also have their own messes.\n",
            "Complex: No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_115%> <LevSim_30%> <WordFreq_80%> <DepTreeDepth_100%> who also have their own messes.\n",
            "Complex: The soul is believed to be eternal. \n",
            "\n",
            "Simple: <NbChars_200%> <LevSim_85%> <WordFreq_100%> <DepTreeDepth_200%> who also have their own messes.\n",
            "Complex: 'Sins' and evil-doings of the devotee are said to fall away of their own accord, the devotee shriven, limitedness even transcended, through the love of God. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> He is the first footballer and the third sportsman to earn $1 billion in their career.\n",
            "Complex: He is the first footballer and only the third sportsman to earn $1 billion in their career. \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_100%> I think they have their own personal pride in terms of wanting to be the best.\" Messi himself denied any rivalry, saying that it was \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\"\n",
            "Complex: I think they have their own personal pride in terms of wanting to be the best.\" Messi has denied any rivalry, and blames the media for creating it, stating that \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano. \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_100%> I think they have their own personal pride in terms of wanting to be the best.\" Messi himself denied any rivalry, saying that it was \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\"\n",
            "Complex: I think they have their own personal pride in terms of wanting to be the best.\" Messi has denied any rivalry, and blames the media for creating it, stating that \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> He is the first footballer and only the third sportsman to earn $1 billion in their career.\n",
            "Complex: He is the first footballer and the third sportsman to earn $1 billion in their career. \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> They are two of the most decorated football players ever, having won a combined 67 trophies (Ronaldo 32, Messi 35) during their senior careers thus far, and have regularly broken the 50-goal barrier in a single season.\n",
            "Complex: They are two of the most decorated football players ever, having won a combined 66 trophies (Ronaldo 32, Messi 34) during their senior careers thus far, and have regularly broken the 50-goal barrier in a single season. \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's physical attributes, goalscoring skills, leadership and influence under pressure is well-appreciated worldwide.\n",
            "Complex: Messi's combination of dribbling, technical, playmaking, and goalscoring skills is often considered among the very best in history, while Ronaldo's leadership and influence under pressure is well-appreciated worldwide. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_85%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_85%> In 2019, Messi took the lead again by earning his sixth Ballon d'Or, finishing just seven points ahead of second-placed Virgil van Dijk, with Ronaldo finishing third. In total, Messi and Ronaldo reached the podium a record twelve times each.\n",
            "Complex: In 2019, Messi took the lead again by earning his sixth Ballon d'Or. He finished just seven points ahead of second-placed Virgil van Dijk, with Ronaldo third. In total, Messi and Ronaldo have both won the World Cup twelve times. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_105%> <DepTreeDepth_85%> Messi during this period won five Pichichi trophies and European Golden Shoe awards (2010, 2012, 2013, 2017 and 2018), while Ronaldo won these prizes four times each (2008, 2011, 2014 and 2015).\n",
            "Complex: Messi during this period won five Pichichi trophies and European Golden Shoe awards (2010, 2012, 2013, 2017 and 2018), while Ronaldo won these prizes thrice each (2011, 2014 and 2015). \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_85%> <WordFreq_100%> <DepTreeDepth_55%> == Relationship between Messi and Ronaldo == In a 2015 interview, Ronaldo commented on the rivalry by saying: \"I think we push each other sometimes in the competition, this is why the competition is so high.\"\n",
            "Complex: == Relationship between Messi and Ronaldo ==  In a 2015 interview, Ronaldo talked about the rivalry. He said: \"Sometimes we push each other a little bit, that's why the competition is so good. \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_100%> I think they have their own personal pride in terms of wanting to be the best.\" Messi has denied any rivalry, and blames the media for creating it, stating that \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\n",
            "Complex: I think they have their own personal pride in terms of wanting to be the best.\" Messi himself denied any rivalry, saying that it was \"only the media, the press, who wants us to be at loggerheads but I've never fought with Cristiano.\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_90%> <WordFreq_95%> <DepTreeDepth_115%> Although it was a bit difficult to see him win trophies, he gave La Liga prestige.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\"\n",
            "Complex: It was a little hard for him to win trophies, but he gave La Liga importance.\" During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_115%> During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi later replied: \"If I get an invitation, why not?\"\n",
            "Complex: During a joint interview at the UEFA Player of the Year ceremony in 2019, Ronaldo said he would like to \"have dinner together in the future\", to which Messi said: \"If I get an invitation, why not?\" \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> == Awards and records == Throughout the existence of the rivalry, the pair have dominated awards ceremonies and broken a multitude of goalscoring records for both club and country, feats which have been described as \"incredible\", \"ridiculous\" and \"remarkable\".\n",
            "Complex: ---  == Awards and records == Throughout the existence of the rivalry, the pair have dominated awards ceremonies and broken a multitude of goalscoring records for both club and country, feats which have been described as \"incredible\", \"ridiculous\", and \"remarkable\", respectively. \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level.\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. \n",
            "\n",
            "Simple: <NbChars_105%> <LevSim_90%> <WordFreq_100%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level.\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_85%> <LevSim_90%> <WordFreq_105%> <DepTreeDepth_65%> In an interview for the France Football, Modrić stated that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Cristiano Ronaldo and Lionel Messi, who are players at another level. Nobody has the right to compare themselves to them.\"\n",
            "Complex: In an interview for the France Football, Modrić said that \"history will say that a Croatian player, representing his small country, won the Ballon d'Or after Lionel Messi and Cristiano Ronaldo. No one should compare themselves to them.\" \n",
            "\n",
            "Simple: <NbChars_80%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_85%> He has also won four national cups, two league cups, six national super cups, two European Super Cups, and four FIFA Club World Cups.\n",
            "Complex: He has also won four national cups, two league cups, six national super cups, and four FIFA Club World Cups. \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> Moreover, Messi was runner-up at three Copa Américas and at the 2014 World Cup, before finally claiming his first major international trophy at the 2021 Copa América where he was named best player and joint top scorer.\n",
            "Complex: Moreover, Messi was runner-up at three Copa Américas and at the 2014 World Cup, before finally claiming his first major international trophy at the 2021 Copa América where he was named joint-best player and top scorer. \n",
            "\n",
            "Simple: <NbChars_95%> <LevSim_100%> <WordFreq_100%> <DepTreeDepth_100%> The term Hindu was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts, including Chaitanya Charitamrita and Chaitanya Bhagavata.\n",
            "Complex: The term Hindu was later used in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts including Chaitanya Charitamrita and Chaitanya Bhagavata. \n",
            "\n",
            "Simple: <NbChars_100%> <LevSim_95%> <WordFreq_100%> <DepTreeDepth_80%> These texts used it to contrast Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century Chaitanya Charitamrita text and the 17th-century Bhakta Mala text using the phrase \"Hindu dharma\".\n",
            "Complex: These texts used to distinguish Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century Chaitanya Charitamrita text and the 17th century Bhakta Mala text using the phrase \"Hindu dharma\". \n",
            "\n",
            "Simple: <NbChars_110%> <LevSim_70%> <WordFreq_85%> <DepTreeDepth_100%> This paragraph is tough to comprehend.\n",
            "Complex: This paragraph is very hard to understand. \n",
            "\n",
            "Simple: <NbChars_90%> <LevSim_70%> <WordFreq_120%> <DepTreeDepth_100%> This paragraph is very hard to understand.\n",
            "Complex: This paragraph is tough to comprehend. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JVgbXvwNtu1"
      },
      "source": [
        "**Add Control Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugItZ0s2N4k_"
      },
      "source": [
        "final_pps = prepend_control_tokens(paraphrases)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIMH3TFBXEtL"
      },
      "source": [
        "**Fine-tune BART for Text Simlplification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxiIFuabXaCF"
      },
      "source": [
        "tokenizer = get_tokenizer()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjHimSPLU-il"
      },
      "source": [
        "dataset = get_dataset(final_pps, tokenizer)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M44IGfhEXfhl"
      },
      "source": [
        "model = get_model(len(tokenizer))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QSEHQb2XRwo"
      },
      "source": [
        "data_collator = get_data_collator(tokenizer, model)\n",
        "args = get_training_arguments(epochs=1, batch_size=8)\n",
        "trainer = get_trainer(model, tokenizer, dataset, data_collator, args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "daAnf0xdK844",
        "outputId": "6d3cdc8d-a19e-4f5a-fbaa-d17015beef41"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 72\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 04:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9, training_loss=0.9259072409735786, metrics={'train_runtime': 278.6859, 'train_samples_per_second': 0.258, 'train_steps_per_second': 0.032, 'total_flos': 20367397158912.0, 'train_loss': 0.9259072409735786, 'epoch': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVgdRbyBZ00U"
      },
      "source": [
        "---"
      ]
    }
  ]
}
